# Fleet Risk Analysis Using Big Data & Power BI  

## ğŸ“Œ Overview  
This project analyzes fleet operations data to identify high-risk drivers, improve compliance, and support data-driven decision making. By leveraging **Hadoop, Hive, Impala, Spark Streaming, Airflow, and Power BI**, the project builds a scalable data pipeline and delivers interactive dashboards with actionable insights for fleet risk management.  

---

## ğŸš€ Key Features  
- **Big Data Pipeline**: Built using **HDFS, Hive, and Impala** to process and query large-scale geolocation datasets.  
- **Real-Time Data Processing**: Explored **Spark Streaming** and **Airflow** for scheduling and streaming analytics to monitor compliance in near real time.  
- **ETL & Transformation**: Processed **100K+ geolocation records** from over **300 trucks and 310 drivers** to engineer risk features.  
- **Interactive Dashboards**: Developed **Power BI dashboards** to visualize fleet KPIs, compliance trends, and high-risk driver profiles.  
- **Actionable Insights**: Delivered **5+ insights** that improved compliance reporting efficiency by **40%**.  

---

## ğŸ› ï¸ Tech Stack  
- **Big Data**: Hadoop (HDFS, Hive, Impala), Spark Streaming, Airflow  
- **ETL & Data Processing**: Hive ETL  
- **Visualization**: Power BI  
- **Languages**: SQL, Python  

---

## ğŸ“Š Results  
- â±ï¸ Reduced query time by **30%** through optimized Hadoop pipeline.  
- ğŸšš Identified high-risk drivers across **300+ trucks and 310 drivers**.  
- ğŸ“ˆ Improved compliance reporting efficiency by **40%** with actionable insights.  

---

## ğŸ“‚ Repository Structure  
```
â”œâ”€â”€ data/                  # Sample or synthetic datasets (if shareable)
â”œâ”€â”€ notebooks/             # Data cleaning, feature engineering, analysis
â”œâ”€â”€ pipeline/              # Hadoop/Hive/Impala pipeline scripts
â”œâ”€â”€ dashboards/            # Power BI dashboard files/screenshots
â”œâ”€â”€ airflow/               # Airflow DAGs (if included)
â”œâ”€â”€ README.md              # Project documentation
```

---

## ğŸ“– How to Run  
1. Set up a **Hadoop cluster** with Hive and Impala installed.  
2. Load sample fleet geolocation data into **HDFS**.  
3. Run Hive ETL scripts in `/pipeline/` to preprocess and transform data.  
4. Use **Impala** to query and aggregate processed data.  
5. Optionally configure **Airflow DAGs** for orchestration and **Spark Streaming** for near real-time updates.  
6. Connect the processed dataset to **Power BI** to build dashboards.  

---

## ğŸ”® Future Enhancements  
- Implement machine learning models for **driver risk prediction**.  
- Integrate **real-time IoT streaming** data feeds.  
- Deploy dashboards via a **web application** for broader accessibility.  

---

## ğŸ‘¤ Author  
**Sai Nikhel Rangala**  
- ğŸ“§ [sainikhel1@gmail.com](mailto:sainikhel1@gmail.com)  
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/sai-nikhel-rangala)  
