# Fleet Risk Analysis Using Big Data & Power BI  

## 📌 Overview  
This project analyzes fleet operations data to identify high-risk drivers, improve compliance, and support data-driven decision making. By leveraging **Hadoop, Hive, Impala, Spark Streaming, Airflow, and Power BI**, the project builds a scalable data pipeline and delivers interactive dashboards with actionable insights for fleet risk management.  

---

## 🚀 Key Features  
- **Big Data Pipeline**: Built using **HDFS, Hive, and Impala** to process and query large-scale geolocation datasets.  
- **Real-Time Data Processing**: Explored **Spark Streaming** and **Airflow** for scheduling and streaming analytics to monitor compliance in near real time.  
- **ETL & Transformation**: Processed **100K+ geolocation records** from over **300 trucks and 310 drivers** to engineer risk features.  
- **Interactive Dashboards**: Developed **Power BI dashboards** to visualize fleet KPIs, compliance trends, and high-risk driver profiles.  
- **Actionable Insights**: Delivered **5+ insights** that improved compliance reporting efficiency by **40%**.  

---

## 🛠️ Tech Stack  
- **Big Data**: Hadoop (HDFS, Hive, Impala), Spark Streaming, Airflow  
- **ETL & Data Processing**: Hive ETL  
- **Visualization**: Power BI  
- **Languages**: SQL, Python  

---

## 📊 Results  
- ⏱️ Reduced query time by **30%** through optimized Hadoop pipeline.  
- 🚚 Identified high-risk drivers across **300+ trucks and 310 drivers**.  
- 📈 Improved compliance reporting efficiency by **40%** with actionable insights.  

---

## 📂 Repository Structure  
```
├── data/                  # Sample or synthetic datasets (if shareable)
├── notebooks/             # Data cleaning, feature engineering, analysis
├── pipeline/              # Hadoop/Hive/Impala pipeline scripts
├── dashboards/            # Power BI dashboard files/screenshots
├── airflow/               # Airflow DAGs (if included)
├── README.md              # Project documentation
```

---

## 📖 How to Run  
1. Set up a **Hadoop cluster** with Hive and Impala installed.  
2. Load sample fleet geolocation data into **HDFS**.  
3. Run Hive ETL scripts in `/pipeline/` to preprocess and transform data.  
4. Use **Impala** to query and aggregate processed data.  
5. Optionally configure **Airflow DAGs** for orchestration and **Spark Streaming** for near real-time updates.  
6. Connect the processed dataset to **Power BI** to build dashboards.  

---

## 🔮 Future Enhancements  
- Implement machine learning models for **driver risk prediction**.  
- Integrate **real-time IoT streaming** data feeds.  
- Deploy dashboards via a **web application** for broader accessibility.  

---

## 👤 Author  
**Sai Nikhel Rangala**  
- 📧 [sainikhel1@gmail.com](mailto:sainikhel1@gmail.com)  
- 🔗 [LinkedIn](https://www.linkedin.com/in/sai-nikhel-rangala)  
